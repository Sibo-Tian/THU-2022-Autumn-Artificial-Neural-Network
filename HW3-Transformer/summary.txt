########################
# Additional Files
########################
# tokenizer
# train_test
# __pycache__
# data

########################
# Filled Code
########################
# ../codes/model_tfmr.py:1
            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).reshape(1,1,max_positions, max_positions)

# ../codes/model_tfmr.py:2
        attn_weights = torch.matmul(query, key.transpose(-1, -2))
        len_query = query.shape[-2]
        len_key = key.shape[-2]
        causal_mask = self.bias[:,:, len_key-len_query:len_key, :len_key].bool()

        softmax_layer = nn.Softmax(-1)
        attn_weights = softmax_layer(attn_weights)
        attn_output = torch.matmul(attn_weights, value)

# ../codes/model_tfmr.py:3
        multi_head_shape = tensor.shape[:-1] + (num_heads, attn_head_size)
        tensor = tensor.reshape(multi_head_shape)
        tensor = tensor.permute(0, 2, 1, 3)
        return tensor

# ../codes/model_tfmr.py:4
        tensor = tensor.permute(0, 2, 1, 3)
        merged_shape = tensor.shape[:-2] + (num_heads * attn_head_size,)
        tensor = tensor.reshape(merged_shape)
        return tensor

# ../codes/model_tfmr.py:5
        hidden_states = attn_output + residual
        residual = hidden_states

        hidden_states = self.ln_2(hidden_states)
        hidden_states = self.mlp(hidden_states)

        hidden_states = hidden_states + residual

# ../codes/model_tfmr.py:6
        if past_key_values is None:
            prefix_len = 0
            past_key_values = tuple([None] * len(self.h))
        else:
            prefix_len = past_key_values[0][0].shape[-2]

        position_ids = torch.arange(prefix_len, prefix_len + input_shape[-1], dtype=torch.long, device=device)
        position_ids = position_ids.unsqueeze(0).reshape((-1,input_shape[-1]))
        position_embeds = self.wpe(position_ids)

# ../codes/model_tfmr.py:7
            shift_logits = lm_logits[:, :-1, :].contiguous()
            shift_labels  = labels[:, 1:].contiguous()
            loss = ce_loss_fct(shift_logits.reshape(-1, shift_logits.shape[-1]), shift_labels.reshape(-1))
            loss = loss.reshape(labels.shape[0],-1)

            pad_pos = torch.eq(shift_labels, PAD_ID).to(torch.float).to(labels.device)
            loss_mask = torch.cat([torch.zeros((shift_labels.shape[0], 1),device=labels.device), pad_pos[:, :-1]],1)
            loss_mask = 1 - loss_mask

            loss = loss * loss_mask
            loss = torch.mean(loss.sum(1) / (loss_mask.sum(1) + 1e-20))

# ../codes/model_tfmr.py:8
                        sorted_logits , sorted_indices= torch.sort(logits, dim=-1, descending= True)
                        cumu_prob = torch.cumsum(F.softmax(sorted_logits, dim=-1),dim=-1)

                        #Preserve the first item that satisfies cumu_prob > top_p, because the item before it has cumu_prob <= top_p,
                        #may below the threshold top_p
                        indices_to_remove = cumu_prob > top_p
                        indices_to_remove = torch.cat([torch.zeros((indices_to_remove.shape[0],1)), indices_to_remove[:,:-1]],dim=1).bool()

                        sorted_indices = sorted_indices + torch.arange(indices_to_remove.shape[0],dtype=torch.long,device=device).unsqueeze(-1) * indices_to_remove.shape[-1]
                        indices_to_remove = torch.masked_select(sorted_indices, indices_to_remove)
                        logits = logits.reshape(-1)
                        logits = torch.index_fill(logits, 0, indices_to_remove, -float("inf"))
                        logits = logits.reshape(sorted_indices.shape)

# ../codes/model_tfmr.py:9
                        sorted_logits, sorted_indices= torch.sort(logits, dim=-1, descending= True)
                        cumu_prob = torch.cumsum(F.softmax(sorted_logits, dim=-1),dim=-1)
                        num_instances, num_classes = logits.shape[0], logits.shape[1]
                        #only this line(indices_to_remove) is different from top-p
                        indices_to_remove = torch.cat([torch.zeros((num_instances,top_k)), torch.ones((num_instances, num_classes - top_k))],dim=1).bool()

                        sorted_indices = sorted_indices + torch.arange(indices_to_remove.shape[0],dtype=torch.long,device=device).unsqueeze(-1) * indices_to_remove.shape[-1]
                        indices_to_remove = torch.masked_select(sorted_indices, indices_to_remove)
                        logits = logits.reshape(-1)
                        logits = torch.index_fill(logits, 0, indices_to_remove, -float("inf"))
                        logits = logits.reshape(sorted_indices.shape)

# ../codes/main.py:1
            tgt_ids = input_ids[:, 1:].contiguous()
            input_ids = input_ids[:, :-1].contiguous()
            loss = loss.reshape((input_ids.shape[0],-1))
            #Compute the loss mask; Note that the first input that prodeces the PAD is valid, therefore we should preserve the first PAD in label, i.e., the -1 in `loss_mask[:,:-1]`
            loss_mask = torch.eq(tgt_ids, PAD_ID).to(float).to(device)
            loss_mask = torch.cat([torch.zeros((loss_mask.shape[0],1),dtype=float,device=device), loss_mask[:, :-1]],1)
            loss_mask = 1 - loss_mask

            loss = loss * loss_mask
            loss = loss.sum(1) / (loss_mask.sum(1) + 1e-20)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/main.py -> ../codes/main.py
# 44 - parser.add_argument('--pretrain_dir', type=str, default='None',
# 44 ?                                                         -    -
# 44 + parser.add_argument('--pretrain_dir', type=str, default=None,
# 163 -     device = "cuda:6" #torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 163 ?              ----------
# 169 +     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

