########################
# Additional Files
########################
# .DS_Store
# train
# __pycache__
# cifar-10_data

########################
# Filled Code
########################
# ../codes/mlp/model.py:1
    def __init__(self, num_features, epsilon=1e-5, momentum=1e-3):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.weight = nn.Parameter(torch.ones(num_features, device = device))
        self.bias = nn.Parameter(torch.zeros(num_features, device = device))
        self.register_buffer('running_mean', torch.zeros(num_features, device = device))
        self.register_buffer('running_var', torch.zeros(num_features, device = device))
        self.epsilon = epsilon
        self.momentum = momentum
        if self.training == False:
            _z = (input - self.running_mean) / torch.sqrt(self.running_var + self.epsilon)
            _output = _z * self.weight + self.bias
            return _output
        _mean = input.mean(dim=0)
        _variance = input.var(dim=0, unbiased=False)
        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * _mean
        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * _variance

        z = (input - _mean) / torch.sqrt(_variance + self.epsilon)
        output = z * self.weight + self.bias
        return output

# ../codes/mlp/model.py:2
        if self.training == False:
            return input
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        multiplier = torch.bernoulli((1 - self.p) * torch.ones_like(input, device = device)) * (1 / (1 - self.p))
        return input * multiplier

# ../codes/mlp/model.py:3
        hidden_dim = 1024
        self.net = nn.Sequential(nn.Linear(3072, hidden_dim),
                            BatchNorm1d(hidden_dim),
                            nn.ReLU(),
                            Dropout(drop_rate),
                            nn.Linear(hidden_dim, 10)
                            )

# ../codes/mlp/model.py:4
        logits = self.net(x)

# ../codes/cnn/model.py:1
    def __init__(self, num_features, epsilon=1e-5, momentum=1e-3):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.weight = nn.Parameter(torch.ones(num_features , device = device))
        self.bias = nn.Parameter(torch.zeros(num_features , device = device))
        self.register_buffer('running_mean', torch.zeros(num_features, device= device))
        self.register_buffer('running_var', torch.zeros(num_features, device= device))
        self.epsilon = epsilon
        self.momentum = momentum
        # input: [batch_size, num_feature_map, height, width]
        if self.training == False:
            _z = (input - self.running_mean[None, :, None, None]) / torch.sqrt(self.running_var[None, :, None, None] + self.epsilon)
            _output = _z * self.weight[None, :,None, None] + self.bias[None, :, None, None]
            return _output
        _mean = input.mean(dim = [0, 2, 3])
        _variance = input.var(dim=[0, 2, 3], unbiased=False)
        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * _mean
        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * _variance

        z = (input - _mean[None, :, None, None]) / torch.sqrt(_variance[None, :, None, None] + self.epsilon)
        output = z * self.weight[None, :, None, None] + self.bias[None, :, None, None]
        return output

# ../codes/cnn/model.py:2
        # input: [batch_size, num_feature_map, height, width]
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.training == False:
            return input
        num_channel = len(input[0])
        multiplier = torch.bernoulli((1 - self.p) * torch.ones(num_channel, device= device)) * (1 / (1 - self.p)) # Drop out entire channel
        return input * multiplier[None, :, None, None]

# ../codes/cnn/model.py:3
        channels_num_1 = 32
        channels_num_2 = 64
        self.net = nn.Sequential(
            nn.Conv2d(3,channels_num_1,kernel_size=5,stride=1,padding=2),
            BatchNorm2d(channels_num_1),
            nn.ReLU(),
            Dropout(drop_rate),
            #Dropout1d(drop_rate),
            nn.MaxPool2d(2,2),
            nn.Conv2d(channels_num_1,channels_num_2,kernel_size=5,stride=1,padding=2),
            BatchNorm2d(channels_num_2),
            nn.ReLU(),
            Dropout(drop_rate),
            #Dropout1d(drop_rate),
            nn.MaxPool2d(2,2)
        )
        self.linear = nn.Linear(8*8*channels_num_2,10)

# ../codes/cnn/model.py:4
        feature = self.net(x)
        feature_flatten = feature.reshape(feature.shape[0],-1)
        logits = self.linear(feature_flatten)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/cnn/model.py -> ../codes/cnn/model.py
# 56 +
# 57 + class Dropout1d(nn.Module):
# 58 +     def __init__(self, p=0.5):
# 59 +         super().__init__()
# 60 +         self.p = p
# 61 +
# 62 +     def forward(self, input):
# 63 +         if self.training == False:
# 64 +             return input
# 65 +         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 66 +         multiplier = torch.bernoulli((1 - self.p) * torch.ones_like(input, device= device)) *  (1 / (1 - self.p))
# 67 +         return input * multiplier

